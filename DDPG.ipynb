{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@Author: Yitao Qiu\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from torch.autograd import Variable\n",
    "from utils.qf_data import normalize,load_observations\n",
    "from environment.QF_env import envs\n",
    "from tools.ddpg.replay_buffer import ReplayBuffer\n",
    "from tools.ddpg.ornstein_uhlenbeck import OrnsteinUhlenbeckActionNoise\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "C_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define actor network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self,product_num, win_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels =  1,\n",
    "            out_channels = 32,\n",
    "            kernel_size = (1,3),\n",
    "            #stride = (1,3)\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels = 32,\n",
    "            out_channels = 32,\n",
    "            kernel_size = (1, win_size-2),\n",
    "            #stride = (1, win_size-2)\n",
    "        )\n",
    "        self.linear1 = nn.Linear((product_num + 1)*1*32, 64)\n",
    "        self.linear2 = nn.Linear(64, 64)\n",
    "        self.linear3 = nn.Linear(64,product_num + 1)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.linear1.weight.data.uniform_(*hidden_init(self.linear1))\n",
    "        self.linear2.weight.data.uniform_(*hidden_init(self.linear2))\n",
    "        self.linear3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        conv1_out = self.conv1(state)\n",
    "        conv1_out = F.relu(conv1_out)\n",
    "        conv2_out = self.conv2(conv1_out)\n",
    "        conv2_out = F.relu(conv2_out)\n",
    "        # Flatten\n",
    "        conv2_out = conv2_out.view(conv2_out.size(0), -1)\n",
    "        fc1_out = self.linear1(conv2_out)\n",
    "        fc1_out = F.relu(fc1_out)\n",
    "        fc2_out = self.linear2(fc1_out)\n",
    "        fc2_out = F.relu(fc2_out)\n",
    "        fc3_out = self.linear3(fc2_out)\n",
    "        fc3_out = F.softmax(fc3_out,dim=1)\n",
    "        \n",
    "        return fc3_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Critic network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, product_num, win_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels =  1,\n",
    "            out_channels = 32,\n",
    "            kernel_size = (1,3),\n",
    "            #stride = (1,3)\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels = 32,\n",
    "            out_channels = 32,\n",
    "            kernel_size = (1, win_size-2),\n",
    "            #stride = (1, win_size-2)\n",
    "        )\n",
    "        self.linear1 = nn.Linear((product_num + 1)*1*32, 64)\n",
    "        self.linear2 = nn.Linear((product_num + 1), 64)\n",
    "        self.linear3 = nn.Linear(64, 1)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.linear1.weight.data.uniform_(*hidden_init(self.linear1))\n",
    "        self.linear2.weight.data.uniform_(*hidden_init(self.linear2))\n",
    "        self.linear3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        # Observation channel\n",
    "        conv1_out = self.conv1(state)\n",
    "        conv1_out = F.relu(conv1_out)\n",
    "        conv2_out = self.conv2(conv1_out)\n",
    "        conv2_out = F.relu(conv2_out)\n",
    "        # Flatten\n",
    "        conv2_out = conv2_out.view(conv2_out.size(0), -1)\n",
    "        fc1_out = self.linear1(conv2_out)\n",
    "        # Action channel\n",
    "        fc2_out = self.linear2(action)\n",
    "        obs_plus_ac = torch.add(fc1_out,fc2_out)\n",
    "        obs_plus_ac = F.relu(obs_plus_ac)\n",
    "        fc3_out = self.linear3(obs_plus_ac)\n",
    "        \n",
    "        return fc3_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_normalizer(observation):\n",
    "    # Normalize the observation into close/open ratio\n",
    "    if isinstance(observation, tuple):\n",
    "        observation = observation[0]\n",
    "    \n",
    "    observation = observation[:, :, 3:4] / observation[:, :, 0:1]\n",
    "    observation = normalize(observation)\n",
    "    return observation\n",
    "\n",
    "def hidden_init(layer):\n",
    "    # Initialize the parameter of hidden layer\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(object):\n",
    "    def __init__(self, env, product_num, win_size, actor_noise, config_file = 'config/config.json'):\n",
    "        \n",
    "        with open(config_file) as f:\n",
    "            self.config = json.load(f)\n",
    "        assert self.config != None, \"Can't load config file\"\n",
    "        \n",
    "        self.env = env\n",
    "        self.actor_noise = actor_noise\n",
    "        self.summary_path ='results/ddpg/'\n",
    "        if C_CUDA:\n",
    "            self.actor = Actor(product_num,win_size).cuda()\n",
    "            self.actor_target = Actor(product_num,win_size).cuda()\n",
    "            self.critic = Critic(product_num,win_size).cuda()\n",
    "            self.critic_target = Critic(product_num,win_size).cuda()\n",
    "        else:\n",
    "            self.actor = Actor(product_num,win_size)\n",
    "            self.actor_target = Actor(product_num,win_size)\n",
    "            self.critic = Critic(product_num,win_size)\n",
    "            self.critic_target = Critic(product_num,win_size)\n",
    "        \n",
    "        self.actor.reset_parameters()\n",
    "        self.actor_target.reset_parameters()\n",
    "        self.critic_target.reset_parameters()\n",
    "        self.actor.reset_parameters()\n",
    "        \n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr = self.config['actor learning rate'])\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr = self.config['critic learning rate'])\n",
    "        \n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    \n",
    "    def act(self, state):\n",
    "        if C_CUDA:\n",
    "            state = torch.tensor(state, dtype=torch.float).unsqueeze(0).cuda()\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "        action = self.actor(state).squeeze(0).cpu().detach().numpy()+ self.actor_noise()\n",
    "        return action\n",
    "    \n",
    "    def critic_learn(self, state, action, predicted_q_value):\n",
    "        actual_q = self.critic(state, action)\n",
    "        if C_CUDA:\n",
    "            target_Q = torch.tensor(predicted_q_value, dtype=torch.float).cuda()\n",
    "        else:\n",
    "            target_Q = torch.tensor(predicted_q_value, dtype=torch.float)\n",
    "        target_Q=Variable(target_Q,requires_grad=True)\n",
    "        td_error  = F.mse_loss(actual_q, target_Q)\n",
    "        self.critic_optim.zero_grad()\n",
    "        td_error.backward()\n",
    "        self.critic_optim.step()\n",
    "        return predicted_q_value,td_error\n",
    "    \n",
    "    def actor_learn(self, state):\n",
    "\n",
    "        loss = -self.critic(state, self.actor(state)).mean()\n",
    "        \n",
    "\n",
    "        self.actor_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.actor_optim.step()\n",
    "        return loss\n",
    "        \n",
    "\n",
    "    \n",
    "    def soft_update(self, net_target, net, tau):\n",
    "        for target_param, param  in zip(net_target.parameters(), net.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "    \n",
    "    def train(self):\n",
    "        num_episode = self.config['episode']\n",
    "        batch_size = self.config['batch size']\n",
    "        gamma = self.config['gamma']\n",
    "        tau = self.config['tau']\n",
    "        self.buffer = ReplayBuffer(self.config['buffer size'])\n",
    "        total_step = 0\n",
    "        writer = SummaryWriter(logdir=self.summary_path)\n",
    "        # Main training loop\n",
    "        for i in range(100):\n",
    "            previous_observation = self.env.reset()\n",
    "            # Normalization\n",
    "            previous_observation = obs_normalizer(previous_observation)\n",
    "            # Reshape\n",
    "            previous_observation = previous_observation.transpose(2, 0, 1)\n",
    "            ep_reward = 0\n",
    "            ep_ave_max_q = 0\n",
    "            \n",
    "            # Keep sampling until done\n",
    "            for j in range (self.config['max step']):\n",
    "                # ================================================\n",
    "        \t\t# 1. Given state st, take action at based on actor\n",
    "        \t\t# ================================================\n",
    "                action = self.act(previous_observation)\n",
    "                # ================================================\n",
    "        \t\t# 2. Obtain reward rt and reach new state st+1\n",
    "                # ================================================\n",
    "                observation, reward, done, _ = self.env.step(action)\n",
    "                observation = obs_normalizer(observation)\n",
    "                # Reshape\n",
    "                observation = observation.transpose(2, 0, 1)\n",
    "                # ================================================\n",
    "        \t\t# 3. Store (st, at, rt, st+1)\n",
    "        \t\t# ================================================\n",
    "                self.buffer.add(previous_observation, action, reward, done, observation)\n",
    "                if self.buffer.size() >= batch_size:\n",
    "                    # ==========================================\n",
    "        \t\t\t# 4. Sample (si,ai,ri,si+1) from the buffer\n",
    "        \t\t\t# ==========================================\n",
    "                    s_batch, a_batch, r_batch, t_batch, s2_batch = self.buffer.sample_batch(batch_size)\n",
    "                    # Convert to torch tensor\n",
    "                    if C_CUDA:\n",
    "                        s_batch = torch.tensor(s_batch, dtype=torch.float).cuda()\n",
    "                        a_batch = torch.tensor(a_batch, dtype=torch.float).cuda()\n",
    "                        r_batch = torch.tensor(r_batch, dtype=torch.float).cuda()#.view(batch_size,-1)\n",
    "                        t_batch = torch.tensor(t_batch, dtype=torch.float).cuda()\n",
    "                        s2_batch = torch.tensor(s2_batch, dtype=torch.float).cuda()\n",
    "                        target_q = self.critic_target(s2_batch,self.actor_target(s2_batch)).cpu().detach()\n",
    "                    else:\n",
    "                        s_batch = torch.tensor(s_batch, dtype=torch.float)\n",
    "                        a_batch = torch.tensor(a_batch, dtype=torch.float)\n",
    "                        r_batch = torch.tensor(r_batch, dtype=torch.float)\n",
    "                        t_batch = torch.tensor(t_batch, dtype=torch.float)\n",
    "                        s2_batch = torch.tensor(s2_batch, dtype=torch.float)\n",
    "                        target_q = self.critic_target(s2_batch,self.actor_target(s2_batch)).detach()\n",
    "                    y_i = []\n",
    "                    for k in range(batch_size):\n",
    "                        if t_batch[k]:\n",
    "                            y_i.append(r_batch[k])\n",
    "                        else:\n",
    "                            y_i.append(r_batch[k] + gamma * target_q[k])\n",
    "                    #y_i = r_batch + gamma * target_q\n",
    "                    # =========================================================\n",
    "        \t\t\t# 6. Update the parameters of Q to make Q(si,ai) close to y\n",
    "        \t\t\t# =========================================================\n",
    "                    predicted_q_value,td_error = self.critic_learn(s_batch, a_batch,np.reshape(y_i, (batch_size, 1)))\n",
    "                    writer.add_scalar('TD error', td_error, global_step=total_step)\n",
    "                    ep_ave_max_q += np.amax(predicted_q_value)\n",
    "                    \n",
    "                    # ================================================================\n",
    "        \t\t\t# 7. Update the parameters of of actor to maximize Q(si,actor(si))\n",
    "        \t\t\t# ================================================================\n",
    "                    actor_loss = self.actor_learn(s_batch)\n",
    "                    writer.add_scalar('Actor loss', actor_loss, global_step=total_step)\n",
    "                    # ===============================================\n",
    "        \t\t\t# 8. Every C steps reset Q^ = Q, actor^ = actor\n",
    "        \t\t\t# ================================================\n",
    "                    self.soft_update(self.critic_target, self.critic, tau)\n",
    "                    self.soft_update(self.actor_target, self.actor, tau)\n",
    "                ep_reward += reward\n",
    "                previous_observation =  observation\n",
    "                total_step = total_step+1\n",
    "                if done or j == self.config['max step'] - 1:\n",
    "                    writer.add_scalar('Q-max', ep_ave_max_q / float(j), global_step=i)\n",
    "                    writer.add_scalar('Reward', ep_reward, global_step=i)\n",
    "                    \n",
    "                    print('Episode: {:d}, Reward: {:.2f}, Qmax: {:.4f}'.format(i, ep_reward, (ep_ave_max_q / float(j))))\n",
    "                    break\n",
    "        print('Finish.')\n",
    "        torch.save(self.actor.state_dict(), model_add+model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    model_add = 'models/'\n",
    "    model_name = 'DDPG_sub'\n",
    "    mode = \"Train\"\n",
    "    steps = 1000\n",
    "    product_num = 9\n",
    "    window_length = 3\n",
    "    action_dim = [10]\n",
    "    train_ratio = 0.8\n",
    "    window_size = 1\n",
    "    feature_num = 4\n",
    "    market_feature = ['Open','High','Low','Close']\n",
    "    product_list = [\"AUDCAD\",\"AUDUSD\",\"EURAUD\",\"EURCAD\",\"EURUSD\",\"GBPUSD\",\"NZDCHF\",\"NZDUSD\",\"USDCHF\"]\n",
    "    \n",
    "    actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
    "    \n",
    "    \n",
    "    env = envs(product_list,market_feature,feature_num,steps,window_length,mode)\n",
    "    ddpg_model = DDPG(env,product_num ,window_length,actor_noise ,config_file='config/config.json')\n",
    "    ###############################################################################################\n",
    "    ddpg_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    actor = Actor(product_num = 9,win_size = 3).cuda()\n",
    "    actor.load_state_dict(torch.load(model_add+model_name))\n",
    "    return actor\n",
    "    \n",
    "def test_model(env, model):\n",
    "    observation, info = env.reset()\n",
    "    observation = obs_normalizer(observation)\n",
    "    observation = observation.transpose(2, 0, 1)\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    counter = 0 \n",
    "    while not done:\n",
    "        observation = torch.tensor(observation, dtype=torch.float).unsqueeze(0).cuda()\n",
    "        action = model(observation).squeeze(0).cpu().detach().numpy()\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        ep_reward += reward\n",
    "        observation = obs_normalizer(observation)\n",
    "        observation = observation.transpose(2, 0, 1)\n",
    "    #print(ep_reward)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_add ='Data/'\n",
    "train_ratio = 0.8\n",
    "window_size = 1\n",
    "window_length = 3\n",
    "market_feature = ['Open','High','Low','Close']\n",
    "feature_num = 4\n",
    "product_list = [\"AUDCAD\",\"AUDUSD\",\"EURAUD\",\"EURCAD\",\"EURUSD\",\"GBPUSD\",\"NZDCHF\",\"NZDUSD\",\"USDCHF\"]\n",
    "\n",
    "observations,ts_d_len = load_observations(window_size,market_feature,feature_num,product_list)\n",
    "\n",
    "train_size = int(train_ratio*ts_d_len)\n",
    "test_observations = observations[int(train_ratio * observations.shape[0]):]\n",
    "test_observations = np.squeeze(test_observations)\n",
    "test_observations = test_observations.transpose(2, 0, 1)\n",
    "mode = \"Test\"\n",
    "steps = 405\n",
    "env = envs(product_list,market_feature,feature_num,steps,window_length,mode,start_index=train_size+282,start_date='2019-6-25')\n",
    "model = load_model()\n",
    "test_model(env,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
